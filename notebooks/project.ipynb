{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import copy\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import torch\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DIR_PATH = Path.cwd().parent\n",
    "sys.path.append(str(DIR_PATH))\n",
    "\n",
    "\n",
    "from Models import MLP, VanillaGNN, GCN, GCONV, GAT, GIN\n",
    "\n",
    "from Dataset.data_loader import get_data\n",
    "from Dataset.preprocessing import scale_adj, pad_ohe_features, scale_feature, get_train_val_test_masks\n",
    "from Dataset import DatasetTabular, DatasetGraph\n",
    "from Dataset.graph import to_data_list\n",
    "\n",
    "from Visualization.eda import histogram_classes, feature_bar_plots, violin_plots, corr_mtrx\n",
    "from Visualization.graph import vis_graph, plot_graphs\n",
    "\n",
    "from utils import set_seed, set_device\n",
    "\n",
    "%matplotlib inline\n",
    "# reload modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=42\n",
    "set_seed(seed)\n",
    "device = set_device()\n",
    "\n",
    "data_type = 'balanced' # 'unbalanced' \n",
    "DATA_PATH = DIR_PATH / 'Data'\n",
    "fig_path = DIR_PATH / f'Figures/{data_type}'\n",
    "os.makedirs(fig_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the adjacency matrices and the data\n",
    "A, data = get_data(DATA_PATH, data_type)  \n",
    "\n",
    "# Print information about the dataset\n",
    "S, N = A.shape[0], A.shape[1]\n",
    "\n",
    "display(data.head(5))\n",
    "\n",
    "# store features in x (S, N, M) and labels in y (S, 1) \n",
    "x, y = np.expand_dims(data.iloc[:, 3:].values, axis=-1), np.expand_dims(data.iloc[:,2].values, axis=-1)\n",
    "M = x.shape[-1]\n",
    "\n",
    "print(f'Number of graphs: {S}')\n",
    "print(f'Number of nodes: {N}')\n",
    "print(f'Number of features per node: {M}')\n",
    "\n",
    "# Check if every graph is connected\n",
    "print(f'All connected: {all(nx.is_connected(nx.from_numpy_array(G)) for G in A)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_degree_centrality(A: np.array) -> np.float64:\n",
    "  \"\"\"\n",
    "    Calculate the average degree centrality of an adjacency matrix.\n",
    "  \"\"\"\n",
    "  return np.mean(list(dict(nx.degree_centrality(nx.from_numpy_array(A))).values()))\n",
    "\n",
    "def get_avg_closeness_centrality(A: np.array) -> np.float64:\n",
    "  \"\"\"\n",
    "    Calculate the average closeness centrality of an adjacency matrix.\n",
    "  \"\"\"\n",
    "  return np.mean(list(dict(nx.closeness_centrality(nx.from_numpy_array(A))).values()))\n",
    "\n",
    "def get_avg_betweenness_centrality(A: np.array) -> np.float64:\n",
    "  \"\"\"\n",
    "    Calculate the average betweenness centrality of an adjacency matrix.\n",
    "  \"\"\"\n",
    "  return np.mean(list(dict(nx.betweenness_centrality(nx.from_numpy_array(A))).values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the adjacency matrices for each of the classes\n",
    "A_pos = A[np.where(y==0)[0]]\n",
    "A_neg = A[np.where(y==1)[0]]\n",
    "\n",
    "# Compute average degree centrality for each sample per class\n",
    "deg_pos_mean = [get_avg_degree_centrality(a) for a in A_pos]\n",
    "deg_neg_mean = [get_avg_degree_centrality(a) for a in A_neg]\n",
    "\n",
    "# Compute average closeness centrality for each sample per class\n",
    "cls_pos_mean = [get_avg_closeness_centrality(a) for a in A_pos]\n",
    "cls_neg_mean = [get_avg_closeness_centrality(a) for a in A_neg]\n",
    "\n",
    "# Compute average betweenness centrality for each sample per class\n",
    "btw_pos_mean = [get_avg_betweenness_centrality(a) for a in A_pos]\n",
    "btw_neg_mean = [get_avg_betweenness_centrality(a) for a in A_neg]\n",
    "\n",
    "\n",
    "# Combine data and metrics into lists\n",
    "pos_data = [deg_pos_mean, cls_pos_mean, btw_pos_mean]\n",
    "neg_data = [deg_neg_mean, cls_neg_mean, btw_neg_mean]\n",
    "metrics = ['Degree Centrality', 'Closeness Centrality', 'Betweenness Centrality']\n",
    "binranges = [(0.2, 0.5), (0.45, 0.75), (0, 0.1)]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "# Iterate over the data and metrics to create each subplot\n",
    "for i, (pos, neg, metric, binrange) in enumerate(zip(pos_data, neg_data, metrics, binranges)):\n",
    "        histogram_classes(pos, neg, metric, ax=axes[i], bins=50, binrange=binrange)  \n",
    "\n",
    "fig.tight_layout()  # Adjust subplots to fit into figure area.\n",
    "fig.savefig(fig_path / 'histogram.png')  # Save the figure before showing it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently, we can already differentiate between positive and negative based on just the graph structure. **A perfect classifier just using a threshold**, do we even need any GNNs after this? We will further analyze the node features to see if there is a similar way to distinguish between the classes and apply some SOTA GNN models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the bars of mean and with std values\n",
    "x_pos = x[np.where(y==0)[0]].squeeze()\n",
    "x_neg = x[np.where(y==1)[0]].squeeze()\n",
    "\n",
    "save_path = fig_path / 'bar_plot.png'\n",
    "feature_bar_plots(x_pos, x_neg, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the violin plot of the features\n",
    "df_melted = pd.melt(data, id_vars=['class'], \n",
    "                    value_vars=[f'Prot{i}' for i in range(1, N+1)], \n",
    "                    var_name='Protein', value_name='Expression')\n",
    "                                                                                                                                                                                                                                                                                                                                                                                                   \n",
    "save_path = fig_path / 'violins.png'\n",
    "violin_plots(df_melted, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the correlation between the features\n",
    "corr = data.iloc[:, 2:].corr()\n",
    "\n",
    "save_path = fig_path / 'correlation.png'\n",
    "corr_mtrx = corr_mtrx(corr, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features seem to have a Gaussian distribution. They do not provide a strong correlation with the **class** target, although some features, such as **Prot18**, **Prot11** seem to be more correlated, which is confirmed by the violin plats, which show a small difference in the mean values per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize one graph and color the nodes according to the feature values\n",
    "idx = 0\n",
    "label = 'Positive' if y[idx]==1 else 'Negative'\n",
    "\n",
    "print(f'Graph {idx+1}, {label} class')\n",
    "\n",
    "save_path = fig_path / f'graph_{label}.png'\n",
    "vis_graph(A[idx], x[idx, :, 0], label, save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a few graphs of both classes together\n",
    "plot_graphs(A, x, y, 9, 'Positive')\n",
    "plot_graphs(A, x, y, 9, 'Negative')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALE_ADJ_WEIGHTS = True\n",
    "ADD_OHE_FEATURES = True\n",
    "NORMALIZE_FEATURES = True \n",
    "\n",
    "if SCALE_ADJ_WEIGHTS:\n",
    "    # Scale the weights of the adjacency matrices\n",
    "    A = scale_adj(A)    \n",
    "\n",
    "if ADD_OHE_FEATURES:\n",
    "    # Add the identifiers of each node as a OHE feature\n",
    "    x = pad_ohe_features(x, N)\n",
    "    # Number of features is increased by the number of nodes\n",
    "    M += N\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "train_mask, val_mask, test_mask = get_train_val_test_masks(S, y)\n",
    "\n",
    "if NORMALIZE_FEATURES: # TODO: Check this \n",
    "    # Standardize the features from a list features_id \n",
    "    features_id = [0] # list of features (columns) you want to scale\n",
    "    for feature_id in features_id:\n",
    "        x = scale_feature(x, feature_id, train_mask, val_mask, test_mask)\n",
    "        \n",
    "         # check the distribution\n",
    "        data_scaled = np.hstack((x[:, :, feature_id], y))\n",
    "        \n",
    "        df = pd.DataFrame(data=data_scaled, columns=list(data.columns[3:]) + [data.columns[2]]) # set the columns in the cirrect order\n",
    "        df_melted1 = pd.melt(df, id_vars=['class'], \n",
    "                            value_vars=[f'Prot{i}' for i in range(1, 21)], \n",
    "                            var_name='Protein', value_name='Expression')\n",
    "        \n",
    "        save_path = fig_path / f'violins_norm_{feature_id}.png'                                                                                                                                                                                                                                                                                                                                                              \n",
    "        violin_plots(df_melted1, save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: ADD SMTH ABOOUT THE FEATRURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Graph Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP\n",
    "As a baseline model, we can firs us a classic MLP architecture, which completely ignores any topology of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create datasets and convert to mini-batches\n",
    "train_loader = DatasetTabular(x[train_mask], y[train_mask], device).to_loader(batch_size=64, shuffle=True)\n",
    "val_loader = DatasetTabular(x[val_mask], y[val_mask], device).to_loader(batch_size=64, shuffle=True)\n",
    "test_loader = DatasetTabular(x[test_mask], y[test_mask], device).to_loader(batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#N_sim = 5\n",
    "#epochs = 120\n",
    "\n",
    "def test_n_times(model_, train_loader, val_loader, test_loader, epochs, N_sim, verbose=True):\n",
    "    \"\"\"Test the model N_times and return the average accuracy.\"\"\"\n",
    "    accs = []\n",
    "    losses = []\n",
    "    for i in range(N_sim):\n",
    "    # Create a copy of the model\n",
    "        model = copy.deepcopy(model_) \n",
    "        # Train\n",
    "        model.fit(train_loader, val_loader, epochs=epochs, verbose=verbose)\n",
    "        # Test\n",
    "        loss, acc = model.test(test_loader)\n",
    "        if verbose:\n",
    "            print(f'Trial {i+1}, test loss: {loss:.2f} | test accuracy: {acc*100:.2f}%')\n",
    "        \n",
    "        # Store the results\n",
    "        accs.append(acc.detach().detach().cpu().numpy())\n",
    "        losses.append(loss.detach().detach().cpu().numpy())\n",
    "\n",
    "    losses, accs = np.array(losses), np.array(accs)\n",
    "    \n",
    "    print(f'\\nFinal accuracy: {np.mean(accs)*100:.2f}%, std: {np.std(accs)*100:.2f}%')\n",
    "    print(f'Final loss: {np.mean(losses):.2f}, std: {np.std(losses):.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP(dim_in=N*M, dim_h=16, num_layers=1).to(device)\n",
    "print(mlp)\n",
    "test_n_times(mlp, train_loader, val_loader, test_loader, 300, 1, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare a graph dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = to_data_list(A, x, y, device)\n",
    "dataset = DatasetGraph(root='../Data', data_list=data_list) # delete the folder to reset\n",
    "\n",
    "train_loader = dataset[train_mask].to_loader(batch_size=64, shuffle=True)\n",
    "val_loader   = dataset[val_mask].to_loader(batch_size=64, shuffle=True)\n",
    "test_loader  = dataset[test_mask].to_loader(batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn = VanillaGNN(dim_in=M, dim_h=16, num_layers=1).to(device)\n",
    "print(gnn)\n",
    "test_n_times(gnn, train_loader, val_loader, test_loader, 500, 1, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn = GCN(dim_in=M, dim_h=16, num_layers=2, dropout=True).to(device) # weights\n",
    "print(gcn)\n",
    "test_n_times(gcn, train_loader, val_loader, test_loader, 300, 1, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GraphConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn = GCONV(dim_in=M, dim_h=16, num_layers=1).to(device)\n",
    "print(gcn)\n",
    "test_n_times(gcn, train_loader, val_loader, test_loader, 500, 1, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gat = GAT(dim_in=M, dim_h=16, num_layers=2).to(device)\n",
    "print(gat)\n",
    "test_n_times(gat, train_loader, val_loader, test_loader, 500, 1, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gin = GIN(dim_in=M, dim_h=16, num_layers=1).to(device)\n",
    "print(gin)\n",
    "test_n_times(gin, train_loader, val_loader, test_loader, 500, 1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 32\n",
    "\n",
    "from tqdm import tqdm\n",
    "from node2vec import Node2Vec\n",
    "\n",
    "x_ = torch.concat((torch.tensor(x, dtype=torch.float32), torch.zeros((x.shape[0], x.shape[1], dim), dtype=torch.float32)), dim=-1).to(device)\n",
    "\n",
    "for i in tqdm(range(A.shape[0])):\n",
    "    # embedding for each graph\n",
    "    G = nx.from_numpy_array(A[i])\n",
    "    node2vec = Node2Vec(G, dimensions=dim, walk_length=20, num_walks=100, p=2, q=1, workers=20, quiet=True)\n",
    "    model = node2vec.fit(window=10, min_count=1, batch_words=4)\n",
    "    x_[i, :, 1:] = torch.tensor(model.wv[np.arange(N)], dtype=torch.float32)\n",
    "    \n",
    "    #print(model.wv[np.arange(N)].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar outpcomes are found in this **stack overflow** discussion [here](https://stackoverflow.com/questions/75752422/gnn-graph-classification-poor-performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GNNExplainer and IntegratedGradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torch_geometric.data import Data\n",
    "#from torch_geometric.loader import DataLoader\n",
    "#from scipy.sparse import coo_matrix\n",
    "\n",
    "#def get_data(dataset, idx):\n",
    "#    return list(map(dataset.__getitem__, idx))\n",
    "\n",
    "#dataset = []\n",
    "#for idx in range(len(A)):\n",
    "#    edge_index = coo_matrix(A[idx])\n",
    "#    sample = Data(x=x[idx], edge_index=np.vstack((edge_index.row, edge_index.col)), edge_weight=edge_index.data, y=y[idx])\n",
    "#    dataset.append(sample)\n",
    "\n",
    "#train_dataset = get_data(dataset, id_train) \n",
    "#val_dataset = get_data(dataset, id_val) \n",
    "#test_dataset = get_data(dataset, id_test) \n",
    "\n",
    "#print(f\"Number of samples: Training {len(train_dataset)} | Validation {len(val_dataset)} | Testing {len(test_dataset)}\")   \n",
    "\n",
    "#train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "#val_loader = DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
    "#test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train standard scaler on training and validation data, then apply to test data\n",
    "#SCALE_DATA = True # Scale or use raw features\n",
    "\n",
    "#if SCALE_DATA:\n",
    "#    scaler = StandardScaler()\n",
    "#    # train the scaler on training and validation\n",
    "#    scaler.fit(np.concatenate((x_train, x_val), axis=0).reshape(-1, M)) \n",
    "#    # apply the scaler to the data (prevent information leak to the test data)\n",
    "#    print(x_train[0][0])\n",
    "#    x_train = scaler.transform(x_train.reshape(-1, M)).reshape(x_train.shape)\n",
    "#    print(x_train[0][0])\n",
    "#    x_val = scaler.transform(x_val.reshape(-1, M)).reshape(x_val.shape)\n",
    "#    X_test = scaler.transform(x_test.reshape(-1, M)).reshape(x_test.shape)\n",
    "\n",
    "    # check the distribution\n",
    "#    data_scaled = np.hstack((\n",
    "#                            np.concatenate((x_train, x_val, x_test), axis=0).squeeze(), \n",
    "#                            np.concatenate((y_train, y_val, y_test))\n",
    "#                            )\n",
    "#                        )\n",
    "#    df = pd.DataFrame(data=data_scaled, columns=list(data.columns[3:]) + [data.columns[2]]) # set the columns in the cirrect order\n",
    "#    df_melted1 = pd.melt(df, id_vars=['class'], \n",
    "#                        value_vars=[f'Prot{i}' for i in range(1, 21)], \n",
    "#                        var_name='Protein', value_name='Expression')\n",
    "#                                                                                                                                                                                                                                                                                                                                                                                                    \n",
    "#    sns.set(style=\"whitegrid\")\n",
    "#    palette = {1: 'skyblue', 0: 'salmon'}\n",
    "#    plt.figure(figsize=(10, 8))\n",
    "#    ax = sns.violinplot(x='Protein', y='Expression', hue='class', data=df_melted1, split=True, inner=\"quart\", palette=palette)\n",
    "#    ax.legend(handles=ax.legend_.legendHandles, labels=['Negative', 'Positive'])\n",
    "#    plt.axhline(y=0, color='b', linestyle='--')\n",
    "#    plt.title('Violin Plots per Class (Scaled)')\n",
    "#    plt.xlabel('Feature (Node)')\n",
    "#    plt.ylabel('Value')\n",
    "#    plt.xticks(rotation=45)\n",
    "#    plt.ylim([-11, 11])\n",
    "#    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#N_sim = 20 # number of you simulate to get the average performance\n",
    "#verbose = True\n",
    "#res_mlp = []\n",
    "#for i in range(N_sim):\n",
    "#    # Create MLP model\n",
    "#    mlp = MLP(dim_in=N*M, dim_h=16).to(device)\n",
    "#    if i == 0:\n",
    "#        print(mlp,'\\n')\n",
    "    \n",
    "#    # Train\n",
    "#    mlp.fit(data_mlp, epochs=150, verbose=verbose)\n",
    "    \n",
    "#    # Test\n",
    "#    acc = mlp.test(data_mlp)\n",
    "#    if verbose:\n",
    "#        print(f'MLP test accuracy: {acc*100:.2f}%')\n",
    "    \n",
    "#    # Store the results\n",
    "#    res_mlp.append(acc.detach().detach().cpu().numpy())\n",
    "\n",
    "#res_mlp = np.array(res_mlp)\n",
    "#print(f'\\nMLP final accuracy: {np.mean(res_mlp)*100:.2f}%, std: {np.std(res_mlp)*100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
